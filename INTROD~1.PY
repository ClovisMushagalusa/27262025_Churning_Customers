# -*- coding: utf-8 -*-
"""Introduction_to_AI_Assignment_3_Churning_Customers.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EhaqcAYeCHidEmOFV1GA-TJpvWnErKa6
"""

from google.colab import drive
drive.mount('/content/drive')

"""**Importation of Important Libraries**"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, roc_auc_score, make_scorer
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import Pipeline
from xgboost import XGBRegressor
from xgboost import plot_importance
from matplotlib import pyplot

"""**Reading the data to be used for the model.**"""

dataset = pd.read_csv('/content/drive/MyDrive/Introduction_To_AI/CustomerChurn_dataset.csv')

dataset

"""**Cleaning the dataset**

"""

# Drop the customerID column as it obviously has no impact on the model's performance.
columns_to_drop =['customerID']
dataset = dataset.drop(columns_to_drop, axis = 1)
dataset

# Display the first few rows of the dataset and have access to all the attributes
dataset.head()

# Knowing more about the dataset
dataset.info()

# Checking the data types of each column in the dataset.
data_types = dataset.dtypes
data_types

# Checking if the dataset has NaNs in any of its attributes
checking_null = dataset.isnull().sum()
checking_null

# Identifying all numeric attributes for encoding purposes
To_be_Encoded = ['gender', 'Partner',	'Dependents', 'PhoneService',	'MultipleLines',	'InternetService',	'OnlineSecurity',	'OnlineBackup',	'DeviceProtection',	'TechSupport',	'StreamingTV',	'StreamingMovies',	'Contract',	'PaperlessBilling',	'PaymentMethod', 'Churn']

# Encoding the non-numeric attributes.
for non_numeric_attribute in To_be_Encoded:
  dataset[non_numeric_attribute],_=pd.factorize(dataset[non_numeric_attribute])

dataset

# Checking the data types of each column in the dataset.
data_types = dataset.dtypes
data_types

# The "TotalCharges" attribute has numeric values. However it is said to be of type object.

# Converting the "TotalCharges" attribute from object type to float type (numeric type)
dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'], errors='coerce')

# Checking the NaN values in the dataset after the convertion.
checking_null = dataset.isnull().sum()
checking_null

# Filling the NaN with the mean.
dataset['TotalCharges'].fillna(dataset['TotalCharges'].mean(), inplace=True)

# Checking the NaN values in the dataset after filling in the mean.
checking_null = dataset.isnull().sum()
checking_null

"""**Splitting the data into the input (X) and output (Y)**"""

# Splitting the dataset into input and output
Input_Data = dataset.drop('Churn', axis = 1)
Output_Data = pd.DataFrame(dataset['Churn'], columns = ['Churn'])

Input_Data

Output_Data

"""**2. Exploratory Data Analysis**"""

# Displaying basic information about the dataset
dataset.info()

# Summarizing statistics
dataset.describe()

# Visualizing the distribution of the target variable / Output (Churn)
sns.countplot(x='Churn', data=dataset)
plt.title('Distribution of Churn')
plt.show()

# Visualizing correlations between numerical features
correlation_matrix = dataset.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Exploring individual features to identify patterns:

# Numerical Features
dataset.hist(figsize=(10, 10), bins=20)
plt.show()

# Categorical Features
for cat_feature in Input_Data:
    sns.countplot(x=cat_feature, hue='Churn', data=dataset)
    plt.show()

"""Feature Importance"""

Input_Data_Attribute = Input_Data.columns

# fit model no training data
model = XGBRegressor()
model.fit(Input_Data, Output_Data)
# plot feature importance
importances = model.feature_importances_


# Print or use feature importances
sorted_indices = np.argsort(importances)[::-1]
for index in sorted_indices:
   print(f"'{Input_Data_Attribute[index]}', '{importances[index]}'")
   print()

Input_Features_To_Drop = ['StreamingMovies', 'PaymentMethod', 'TotalCharges', 'MultipleLines', 'Dependents', 'DeviceProtection', 'SeniorCitizen', 'Partner', 'gender',]

Final_Input_Data = Input_Data.drop(Input_Features_To_Drop, axis = 1)

Final_Input_Data

# Splitting the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(Final_Input_Data, Output_Data, test_size=0.2, random_state=42)

# Standardizing the data to ensure our features have similar scales.

scaler = StandardScaler()
Input_train_scaled = scaler.fit_transform(X_train)
Input_test_scaled = scaler.transform(X_test)

"""**Multi-Layer Perceptron model using the Functional API**

First Model
"""

# Defining the input layer
input_layer = Input(shape=(Input_train_scaled.shape[1],))

# Adding hidden layers with dropout
hidden_1 = Dense(300, activation='relu')(input_layer)
hidden_2 = Dense(256, activation='relu')(hidden_1)
dropout_1 = Dropout(0.1)(hidden_2)

hidden_3 = Dense(128, activation='relu')(dropout_1)
hidden_4 = Dense(64, activation='relu')(hidden_3)
dropout_2 = Dropout(0.2)(hidden_4)

hidden_5 = Dense(32, activation='relu')(dropout_2)
hidden_6 = Dense(16, activation='relu')(hidden_5)
hidden_7 = Dense(8, activation='relu')(hidden_6)
hidden_8 = Dense(4, activation='relu')(hidden_7)
dropout_3 = Dropout(0.2)(hidden_8)

# Output layer
output_layer = Dense(1, activation='sigmoid')(dropout_3)

# Creating the model
model = Model(inputs=input_layer, outputs=output_layer)

# Compiling the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Displaying the model summary
model.summary()

#Training the model
model.fit(Input_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(Input_test_scaled, y_test))

# Evaluating the model on the test set
Output_prediction_probability = model.predict(Input_test_scaled)

Output_prediction = (Output_prediction_probability > 0.5).astype(int)  # Converting probabilities to binary predictions

# Calculating accuracy
accuracy = accuracy_score(y_test, Output_prediction)
print(f'Accuracy: {accuracy:.5f}')

# Calculating AUC
aucu = roc_auc_score(y_test, Output_prediction_probability)
print(f'AUC: {aucu:.5f}')

# Evaluating the model on the test set
accuracy = model.evaluate(X_test, y_test)[1]
print(f'Test Accuracy: {accuracy * 100:.2f}%')

"""Second Model"""

# Create an MLP pipeline with a scaler
mlp = MLPClassifier(max_iter=100)  # You can customize other hyperparameters

pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('mlp', mlp)
])

# Define the parameter grid
param_grid = {
    'mlp__hidden_layer_sizes': [(50,),(100,)],
    'mlp__activation': ['relu', 'tanh'],
    'mlp__alpha': [0.0001, 0.001],
}

# Use AUC as the scoring metric
scorer = make_scorer(roc_auc_score)

# Perform Grid Search with Cross-Validation
grid_search = GridSearchCV(pipeline, param_grid, scoring=scorer, cv=5)
grid_search.fit(X_train, y_train)

# Get the best hyperparameters and model
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_

# Evaluate the best model on the test set
y_pred_prob = best_model.predict_proba(X_test)[:, 1]
auc = roc_auc_score(y_test, y_pred_prob)

print("Best Hyperparameters:", best_params)
print("AUC on Test Set:", auc)

print(f'Test Accuracy: {auc * 100:.4f}%')

import pickle
pickle.dump(best_model, open("Churning_Customer_model.sav", 'wb'))



